\section{Обзор существующих решений}
\label{sec:Chapter2} \index{Chapter2}

% Описание задачи и её типов
Распознавание рукописного текста (HTR) позволяет переводить рукописный текст в цифровой формат.
Основная сложность задачи заключается в значительной вариативности рукописного текста.
Почерк каждого человека отличается от других людей (межличностная изменчивость),
и даже один и тот же человек пишет одно и то же слово по-разному (внутриличностная изменчивость)~\cite{sueiras2021continuous}.
Такие факторы, как скорость письма, размер шрифта, тип используемой бумаги и ручки и даже эмоциональное состояние,
еще больше увеличивают эту изменчивость.

В зависимости от характера исходных данных существуют два основных направления исследования задачи.
С одной стороны, решается задача распознавания рукописного текста на бумажной странице,
отсканированной в цифровое изображение, называемая автономным (офлайн) распознаванием рукописного текста.
С другой стороны, существует так называемое онлайн-распознавание рукописного ввода,
заключающееся в распознавании рукописного текста по непрерывным данным о положениях $(x, y)$ ручки,
полученных при написании этого текста.
Эти данные обычно получают путем ввода непосредственно на сенсорном экране.
Мы ограничимся исследованием задачи офлайн распознавания рукописного текста.

% распознавание рукописного текста в контексте анализа документов в целом
Проблема распознавания рукописного текста в автономном режиме открывает несколько направлений исследований, которые еще далеко не полностью решены.
Для распознавания рукописного текста в документе необходимо, чтобы изначально были определены части изображения, содержащие рукописный текст.
Кроме того, при необходимости текст может быть сегментирован в строки и слова.
Изображение текста может встречаться в таблицах или формах, перекрываться с другими элементами страницы, такими как изображения, схемы или графики.
Текст может быть написан на разных языках с разными наборами символов (т.е. алфавитами), например на китайском, арабском или японском.

Нахождение различных типов элементов (например, рукописный текст, печатный текст, графики и т.д.)
в отсканированных изображениях документов называется \textit{анализом макета} (layout analysis).
Это отдельная сложная и до сих пор нерешённая задача, которая широко исследуется в настоящее время~\cite{binmakhashen2019document}.

% сегментация текста
После нахождения текста на изображении, его распознавание обычно включает в себя несколько шагов~\cite{plamondon2000online}.
В самом общем случае изображение с текстом может содержать параграф, состоящий из нескольких строк.
В этом случае чаще всего текст разбивается на строки.
Однако сегментация строк не является тривиальной из-за наклона строк, наклона символов внутри строки и того факта,
что некоторые символы в соседних строках могут перекрываться другими.
Модели распознавания могут быть применены непосредственно к текстовым строкам, либо их можно разбить на слова, и распознавание выполнять на уровне слов.
Большинство современных алгоритмов HTR можно применять либо к строкам, либо к словам, которые являются частным случаем коротких строк.

% наборы данных и аугментация
Для обучения моделей распознавания рукописного текста с использованием методов глубокого обучения
требуются наборы данных изображений рукописного текста, должным образом аннотированные текстом, присутствующим в каждом изображении.
Существует несколько общедоступных наборов данных, некоторые из них содержат изображения отдельных символов,
другие содержат изображения слов, строк и абзацев.
В любом случае объем аннотированных данных, доступных для обучения моделей, ограничен и требует рассмотрения возможности
использования стратегий увеличения данных (аугментации) для обучения моделей.

В следующих секциях описано, каким образом измеряется качество работы методов распознавания рукописного текста.
Кроме того, описаны наборы данных с рукописным текстом на русском языке, а также методы предобработки и аугментации данных.
В заключение, перечисляются основные архитектуры нейросетевых моделей, применяемых для решения задачи распознавания рукописного текста.
Модели в основном разрабатываются для текстов на английском языке, тем не менее существуют решения для русского языка,
которые также описаны в контексте других моделей.


\subsection{Метрики оценки качества}
\label{subsec:evaluation-metrics}

Двумя основными метриками, обычно используемыми для оценки моделей распознавания рукописного текста на уровне слов и строк,
являются \textit{частота ошибок символов} (Character Error Rate, CER) и \textit{частота ошибок слов} (Word Error Rate, WER).

CER измеряет расстояние Левенштейна~\cite{levenshtein1966binary} между предсказанной и реальной последовательностью символов слова.
Расстояние Левенштейна, также иногда называемое расстоянием редактирования,
представляет собой метрику для измерения разницы между двумя последовательностями.
Неформально расстояние Левенштейна между двумя словами (предсказание модели и реальное слово) --
это минимальное количество вставок, удалений или замен, необходимых для преобразования предсказания в правильное слово,
делённое на длину правильного слова, как показано в уравнении~(\ref{eq:cer}).

\begin{equation}
    \label{eq:cer}
    CER(prediction,real)=\frac{substitutions+insertions+deletes}{len(real)}
\end{equation}

Частота ошибок в словах (WER) определяется аналогично CER путем вычисления минимального количества вставок, замен и удалений слов,
необходимых для перехода от текстовой строки, предсказанной моделью, к реальной текстовой строке.

В некоторых работах наравне с частотой ошибок символов и частотой ошибок слов используется \textit{точность} (accuracy).
Данная метрика используется для любых текстовых строк, содержащих как слова, так и предложения.
Точность описывается формулой~(\ref{eq:accuracy}), в которой под равенством подразумевается полное совпадение двух строк.
Данная метрика позволяет оценить качество модели более грубо, так как ошибка в одном символе сильно понижает результирующее значение.

\begin{equation}
    \label{eq:accuracy}
    accuracy(prediction,real)=\frac{\sum_{i=1}^N pred_i = real_i}{N}
\end{equation}


\subsection{Наборы данных}
\label{subsec:datasets}

Существует не так много общедоступных наборов данных на русском языке для обучения моделей и сравнения результатов.
На текущий момент известно два набора данных со словами на кириллице:
\begin{itemize}
    \item Cyrillic Handwriting Dataset\footnote{\url{https://www.kaggle.com/datasets/constantinwerner/cyrillic-handwriting-dataset}};
    \item HKR\footnote{\url{https://github.com/abdoelsayed2016/HKR_Dataset}}.
\end{itemize}

Кроме того, найдено два набора данных русских символов, являющиеся не очень популярными:
\begin{itemize}
    \item CoMNIST\footnote{\url{https://github.com/GregVial/CoMNIST}};
    \item База сегментированных рукописных символов\footnote{\url{https://drive.google.com/folderview?id=0B0EQUc5HmgcGS0l2RDlKenlpNnc&usp=sharing}}.
\end{itemize}

Основная информация о наборах данных представлена в таблице~\ref{tab:datasets}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|p{4cm}|p{7cm}|p{3cm}|}
        \hline
        \textbf{Название} & \textbf{Описание} & \textbf{Размер} \\
        \hline
        \hline
        Cyrillic Handwriting Dataset & Набор русских текстов длиной $\leqslant$ 40 символов, собранный из различных интернет-ресурсов & train=72286 test=1544 \\
        \hline
        HKR~\cite{nurseitov2021handwritten} & Набор из русских (95\%) и казахских (5\%) слов и предложений: ключевые слова, поэмы и алфавит & train=45470 val=9359 test1=5057 test2=5057 \\
        \hline
        CoMNIST & Символы -- русские заглавные буквы, собран при помощи краудсорсинга & более 28000 \\
        \hline
        База сегментированных рукописных символов & Обширная база из строчных и прописных рукописных символов, а также цифр & всего 120750 символов \\
        \hline
    \end{tabular}
    \caption{Наборы данных с кириллицей}
    \label{tab:datasets}
\end{table}

\paragraph{Cyrillic Handwriting Dataset.}{Данный набор данных был опубликован в 2022 году, поэтому еще нет работ, содержащих результаты его обработки.
Этот набор очень интересен с точки зрения разнообразия данных: в него входят студенческие конспекты, заполненные формы, электронные рукописные документы.
Изображения содержат раличного рода шумы и неоднородный фон, в некоторых случаях на изображение одного слова попадают части другого или разлиновка листа.
Набор содержит как слова, так и предложения, некоторые метки могут содержаться несколько раз (например, слово <<что>>), но есть и уникальные.
Согласно табице~\ref{tab:datasets} набор данных состоит из более 70 тысяч обучающих примеров, что также является его существенным достоинством.
Кроме того, он является открытым как для научных исследований, так и для коммерческого использования.
Примеры изображений, встречающиеся в наборе данных, представлены на рисунке~\ref{fig:cyrillic-examples}}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{img/cyrillic}
    \caption{Примеры изображений набора Cyrillic Handwriting Dataset}
    \label{fig:cyrillic-examples}
\end{figure}

\paragraph{HKR.}{Наиболее популярным набором данных, упоминаемым в научной литературе, является набор казахских и русских слов и предложений~\cite{nurseitov2021handwritten}.
Всего в нем содержится более 60 тысяч изображений слов и предложений, написанных примерно 200 различными почерками.
Он создавался путем заполнения однотипных форм, поэтому имеет одну особенность -- в нем содержится большое количество копий одного и того же текста, написанного разными почерками.
Отдельно авторами дается разбиение набора данных\footnote{\url{https://github.com/bosskairat/Dataset}} на тренировочный, валидационный и два тестовых набора.
Первый тестовый набор содержит слова, которых нет в тренировочном наборе, но написанные почерками, присутствующими в тренировочном наборе.
Напротив, второй тестовый набор содержит слова, которые есть в тренировочном наборе, но написанные <<новыми>> почерками.
Эта особенность позволяет провести анализ того, на что в большей степени обращает внимание обучаемая модель: новые способы написания символов или новые сочетания.
Несмотря на значительный объем набора, изображения в нем достаточно хорошего качества и относительно однообразны.
Таким образом, произвольное изображение рукописного текста из <<реального мира>> совершенно не похоже на то, что содержится в описанном наборе данных.
В добавление к этому, использование данного набора ограничивается научными исследованиями, для его использования в коммерческих целях необходимо обратиться к его авторам.
Примеры изображений, встречающиеся в наборе данных, представлены на рисунке~\ref{fig:hkr-examples}}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{img/hkr}
    \caption{Примеры изображений набора HKR}
    \label{fig:hkr-examples}
\end{figure}

\paragraph{Символьные наборы данных.}{Наборы рукописных символов могут пригодиться при распознавании рукописных текстов, если используется обучение модели классификации символов.
Достаточно известным набором является собранный добровольцами CoMNIST, который имеет несколько недостатков.
Он содержит только заглавные символы, соответственно область его применения резко сужается.
Более того, при разметке символы пишутся людьми на экране телефона, планшета или ноутбука, что сильно отличается от написанных текстов на бумаге.
Набор создавался методом краудсорсинга, поэтому добавляется проблема корректности собранных данных.

Существует также малоизвестная база сегментированных символов, в которой содержатся все заглавные и строчные символы русского алфавита.
Каждый из символов написан более 1500 различными почерками, также присутствует более 300 экземпляров каждой цифры.
Изображения создавались путем заполнения форм от руки с последующей бинаризацией изображения, поэтому выглядят они менее искусственно.
Несмотря на непопулярность в литературе, у данной базы есть большой потенциал для дальнейших исследований.}


\subsection{Методы предобработки данных}
\label{subsec:preprocessing}

Несмотря на явное различие в написании текста на кириллице и латинице, в рукописных текстах есть некоторое сходство,
которое позволяет применять похожие методы предобработки данных к текстам на разных языках.
В кириллических рукописных текстах аналогично латинским присутствует наклон символов,
текст пишется слева направо -- соответственно может встречаться наклон строк.
Кроме того, в строках могут встречаться символы, выходящие за пределы основной строки --
например, заглавные символы или символы с верхними или нижними петлями.
Текст, как правило, пишется на бумаге, которая может иметь дефекты, некоторые недостатки может иметь и сам рукописный текст.
Поэтому имеет смысл рассматривать методы предобработки данных, используемые для текстов на латинице (в частности, для английского языка).
Далее будут более подробно описаны некоторые методики, позволяющие улучшить качество входных данных и привести их к более нормализованному виду.

\subsubsection{Удаление шума и бинаризация}
\label{subsubsec:binarization}

Оцифрованное изображение рукописного текста подвержено множеству источников шума, поэтому даже человеку иногда сложно его распознать.
Бумага может содержать следы, может быть не совсем белой или испорченной.
Если лист бумаги тонкий, может быть виден также текст, написанный обратной стороне (эффект просвечивания).
В процессе оцифровки на изображении могут появиться артефакты, вызванные, загрязнениями сканера.
Шумоподавление изображения -- это первый шаг в обработке и стандартизации изображения.
Его цель состоит в том, чтобы получить изображение в градациях серого, в котором текст имеет четкие черные штрихи,
а фон не содержит элементов, влияющих на предсказания модели.

Крайний случай устранения шумов на изображении -- бинаризация.
В результате бинаризации изображение становится чёрно-белым, без градаций серого.
Пример предполагаемых результатов работы бинаризации представлен на рисунке~\ref{fig:binarization}.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth]{img/binarization}
    \caption{Пример документов до (слева) и после (справа) бинаризации}
    \label{fig:binarization}
\end{figure}

Существует большое количество работ, посвящённых решению задачи бинаризации документов, а также обзоров методов~\cite{mustafa2018binarization}.
В частности, один из самых простых и эффективных методов -- использование адаптивного порога бинаризации, вычисляющегося для небольших участков изображения.


\subsubsection{Исправление наклона строки и символов}
\label{subsubsec:slope-slant-correction}

\textit{Наклон строки} -- это наклон текстовой строки документа относительно горизонтальной линии.
Как правило, он появляется, когда текст пишется на пустой странице без предварительной разлиновки.
На верхнем изображении рисунка~\ref{fig:slope-slant} показан пример наклона строки с положительным углом.
Коррекция наклона может выполняться на уровне страницы, а также на уровне строки или даже на уровне слова.

\textit{Наклон символов} -- распространённое свойство, используемое в процессе обучения письму.
Именно поэтому он очень популярен и является одним из основных источников вариативности рукописного текста,
что затрудняет его распознавание с помощью автоматических систем.
Идентификация и исправление наклона являются критически важными аспектами распознавания рукописного текста,
поскольку многие алгоритмы распознавания, использующие изображения рукописного текста в качестве входных данных,
обычно используют подход, который анализирует изображение столбец за столбцом.
К таким моделям можно отнести полносвязные нейронные сети (многослойный перцептрон, рекуррентные нейронные сети),
у моделей, использующих свёрточные сети, такой проблемы нет~\cite{sueiras2021continuous}.

В любом случае коррекция наклона символов обеспечивает снижение вариативности рукописного текста,
что облегчает его распознавание вне зависимости от типа используемой модели.
Поэтому исправление наклона символов, наряду с шумоподавлением, является самым распространенным методом предварительной обработки при решении задачи распознавания рукописного текста.
Коррекция наклона символов обычно выполняется в два этапа: определение угла наклона строк, а затем применение преобразования для коррекции этого угла.
Зачастую, исправление наклона символов иногда частично выполняется одновременно с исправлением наклона строк,
поскольку исправление угла наклона строки изменяет угол наклона символов на ту же величину.
Пример исправления наклона строки с последующим исправлением наклона символов показан на рисунке~\ref{fig:slope-slant}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{img/slant-slope}
    \caption{Пример исправления наклона строки и символов (внизу исправленный вариант)}
    \label{fig:slope-slant}
\end{figure}

Одним из самых популярных методов коррекции наклона строки является преобразование Хафа~\cite{duda1972use} из-за его надёжности и простоты.
Однако этот метод является вычислительно сложным.
Поэтому несколько авторов предложили варианты, которые уменьшают размер пространства Хафа~\cite{pal1996improved,boukharouba2017new}, хотя вычислительные затраты остаются высокими.

В качестве альтернативы другие авторы определяют угол наклона строки, оценивая линию, которая лучше всего соответствует набору пикселей на изображении.
Например, в работе~\cite{gupta2014efficient} используется линейная регрессия координат x, y пикселей в основной области текста.

Другой широко используемый метод, как для определения наклона строки, так и для определения наклона символов, основан на профилях проекций, например~\cite{kavallieratou2002skew} и~\cite{pastor2004projection}.
Горизонтальная проекция текстовых пикселей используется для определения угла наклона строки, а вертикальная проекция используется для определения угла наклона символов.
Подобные методы довольно чувствительны к присутствию шума на изображении, и для их применения необходимо сначала использовать методы, упомянутые в разделе~\ref{subsubsec:binarization}.

Помимо метода, использующего профили проекций, существуют и другие методы исправления угла наклона символов.
В работе~\cite{gupta2012novel} предложен алгоритм определения угла наклона, который основан на способности фильтра Габора обнаруживать направленные текстуры.
В той же работе описан второй метод оценки угла наклона, который использует преобразование Фурье для преобразования изображения слова в спектр Фурье.
Повторение точек вдоль заданного направления активирует частотное пространство в его перпендикулярном направлении, и это направление соответствует углу наклона.
Другой пример можно в работе~\cite{vinciarelli2001new}, где метод определения угла наклона символов основан на гипотезе о том,
что изображение слова наклоняется, когда количество столбцов, содержащих непрерывные штрихи, максимально.


\subsection{Расширение обучающих наборов данных}
\label{subsec:augmentation-generation}

Одной из важнейших проблем при решении любых задач с использованием машинного обучения является недостаточный объем обучающих данных.
Аугментация -- техника, применяемая в процессе обучения моделей и позволяющая снизить серьезность данной проблемы.
Аугментация данных позволяет с помощью некоторых преобразований из существующих данных получить новые, несколько отличающиеся от исходных.
Использование этой техники в процессе обучения позволяет получать бесконечное количество каким-то образом различающихся обучающих примеров.
Тем самым аугментация позволяет получить модели, более устойчивые к изменениям входных данных и показывающие более высокое качество решения задачи.

Несмотря на безусловные плюсы использования аугментации при обучении моделей машинного обучения, этот метод не является панацеей и не позволяет полностью избавиться от проблем.
Любое автоматическое преобразование данных, каким бы случайным и естественным оно ни было, не сможет в полной мере заменить новые реальные данные.
Тем не менее, аугментация широко используется с поправкой на то, что многого от нее ожидать не следует.
Распознавание рукописного текста -- не исключение, практически в любой работе используются как минимум простейшие случайные изменения обучающих данных.

Помимо аугментации, для решения некоторых задач существует возможность использовать данные, сгенерированные автоматически -- синтетические данные.
В некоторых случаях можно обойтись только синтетическими примерами, однако в подавляющем большинстве случаев сгенерированные данные
являются лишь частью обучающего корпуса и должны быть максимально разнообразными, чтобы не спровоцировать переобучение модели.
Этот способ также приемлем для расширения набора данных из рукописных слов/предложений, однако его нельзя использовать без добавления реальных данных.

\subsubsection{Аугментация данных}
\label{subsubsec:augmentation}

Как правило, при обучении моделей распознавания рукописного текста применяются достаточно распространённые случайные изменения:
изменение размера изображения, искажение перспективы, размытие и повышение резкости, добавление случайных шумов, растяжение и сжатие изображения,
случайные повороты, эрозия и дилатация, афинные преобразования и упругие деформации~\cite{sueiras2021continuous}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth]{img/aug_local}
    \includegraphics[width=0.5\textwidth]{img/aug_blots}
    \caption{Примеры аугментации с помощью локальных искажений (слева) и сгенерированных <<рукописных>> пятен (справа)}
    \label{fig:augmentation}
\end{figure}

Еще один способ аугментации описан в работе~\cite{wigington2017data}, где используются локальные искажения на основе сетки контрольных точек изображения (см. рисунок~\ref{fig:augmentation}).
Этот метод отличается от простых афинных преобразований тем, что работает по-разному с маленькими частями изображения, а не искажает его целиком.

Помимо случайных шумов, которые не добавляют реализма изображению, используются и другие методы вставки дополнительных <<объектов>>.
В статье~\cite{shonenkov2021stackmix} описывается аугментация методом рисования на изображении пятен, похожих на рукописные зачеркивания (см. рисунок~\ref{fig:augmentation}).
Для этого на изображении выбираются случайные области, где фиксируются произвольные точки, с помощью которых проводятся кривые Безье.

В некоторых работах аугментация выполняется с помощью дополнительных нейронных сетей, которые обучаются вместе с остальными компонентами модели.
Например, в работе~\cite{krishnan2018word} перед остальными слоями используется сеть пространственного преобразования (STN),
позволяющая менять входные данные путем выбора наиболее релевантных областей изображения и преобразования размера, угла поворота и т. п.~\cite{jaderberg2015spatial}.
В другой работе~\cite{luo2020learn} легковесная нейронная сеть используется для поиска наиболее подходящего набора
контрольных точек, над которым затем производится деформация перемещения наименьших квадратов~\cite{schaefer2006image}.

Несмотря на большое количество разнообразных методов, позволяющих исказить изображение и тем самым позволить обученной
модели стать более устойчивой, они всё же не могут предоставить совершенно новые данные.
В реальности, изображения рукописного текста различаются не только искривлениями букв и пятнами, важную роль играют
сочетания разных символов (то есть сам написанный текст) и то, как соединяются символы в слове (особенности почерка).
Существующие методы аугментации не способны изменить ни то, ни другое.

\subsubsection{Генерация данных}
\label{subsubsec:generation}

Вследствие того, что использование методов аугментации, описанных в секции~\ref{subsubsec:augmentation}, недостаточно для
получения необходимого количества разнообразных обучающих примеров, используется также и другой подход.
Он связан с генерацией новых слов и стилей написания в рукописных текстах.
Такой метод вряд ли позволит получить по-настоящему реалистичные и совершенно новые изображения, однако он может дать
существенный прирост в качестве результатов обучаемых моделей.

Один из интересных способов генерации новых слов и предложений в стиле уже имеющихся слов в наборе данных,
предложили авторы статьи~\cite{shonenkov2021stackmix}, которые также предложили вышеописанный способ рисовать пятна с помощью кривых Безье.
Свой метод они назвали StackMix и состоит он в разбиении слов на символы или подслова, а затем составлении из этих кусочков новых слов (см. рисунок~\ref{fig:generation}).
Такой метод не решает проблему соединения символов и создания новых стилей, зато он может помочь в создании рукописных слов,
которых нет в обучающем наборе данных.

Следующий метод~\cite{krishnan2016generating} состоит в генерации изображений рукописного текста с использованием типографских шрифтов.
Авторы отобрали 90 тысяч уникальных английских слов и сгенерировали 90 миллионов изображений слов с помощью 750 рукописных шрифтов.
Подобный набор слов можно использовать для предобучения моделей распознавания рукописного текста,
однако в силу его однообразности потребуется дообучение на реальных данных, чтобы предотвратить переобучение сети.
Пример генерации изображения рукописного текста с помощью шрифтов приведен на рисунке~\ref{fig:generation}.

В настоящее время является широкоисследуемым целый класс методов генерации рукописного текста,
основанных на использовании генеративно-состязательных сетей (GAN)~\cite{goodfellow2020generative}.
Такие модели в своей основе содержат две нейронных сети: одна из них генерирует примеры (генеративная модель),
а другая пытается отличить подлинные образцы от сгенерированных первой сетью (дискриминативная модель).
Наиболее свежими примерами генеративно-состязательных сетей в области генерации изображений рукописного текста являются
ScrabbleGAN~\cite{fogel2020scrabblegan}, GANwriting~\cite{kang2020ganwriting} и TextStyleBrush~\cite{krishnan2023textstylebrush}.
Пример результата работы ScrabbleGAN приведен на рисунке~\ref{fig:generation}.

\begin{figure}[h!]
    \centering
    \textbf{Stackmix}\par\medskip
    \includegraphics[width=0.9\textwidth]{img/aug_stackmix}
    \textbf{Типографский шрифт}\par\medskip
    \frame{\includegraphics[width=0.9\textwidth]{img/aug_fonts}}
    \textbf{Генеративно-состязательная сеть}\par\medskip
    \frame{\includegraphics[width=0.9\textwidth]{img/aug_gan}}
    \caption{Примеры сгенерированных изображений}
    \label{fig:generation}
\end{figure}

\textit{ScrabbleGAN}~\cite{fogel2020scrabblegan} -- сеть, состоящая из генератора, дискриминатора и распознавателя символов (OCR).
В данной архитектуре дискриминатор влияет на качество изображения, а распознаватель на читаемость текста на изображении.
Для обучения сеть использует картинки с текстом и сам текст, стили написания текста меняются с помощью вектора шума,
на который домножается входной вектор закодированного текста.
\textit{GANwriting}~\cite{kang2020ganwriting} обладает более сложной архитектурой, в которой присутствуют генераторы текста и стиля,
соответственно используются дискриминатор, классификатор стиля и распознаватель текста.
Вместо одной картинки с текстом, на вход подается группа картинок с текстом, которая формирует конкретный стиль.
Поэтому сеть позволяет сгенерировать текст в разных стилях, которые можно смешивать друг с другом.
\textit{TextStyleBrush}~\cite{krishnan2023textstylebrush} -- наиболее свежая модель, состоящая из 7 нейронных сетей и принимающая на вход картинку со стилем и текст.
Тут также есть генераторы текста и стиля, дискриминатор, классификатор стиля и OCR-модель.
Дополняется этот набор сетью, преобразующей выход генератора стиля в набор векторов, а также генератором стилизованного текста,
который использует и объединяет результаты работы предыдущих генераторов.
Данная модель очень сложна по своей структуре и способна генерировать изображения только тех стилей, на которых была обучена.

Вышеописанные нейросетевые модели генерации обучались и тестировались для английского языка, соответственно помимо недостатка,
связанного с искуссвенностью получающихся изображений, есть и другой -- трудоемкость задачи переобучения сети для другого языка.
Более того, далеко не все авторы делятся исходным кодом описанных ими моделей.

\subsection{Типы нейросетевых моделей}
\label{subsec:networks-description}

Существует большое количество вариаций архитектур моделей, которые используются в исследованиях для решения задачи распознавания рукописного текста.
Если исключить из рассмотрения некоторые детали, модели можно объединить в следующие группы:
\begin{itemize}
    \item Модель для извлечения признаков с последующим применением скрытой марковской модели (HMM).
    \item Свёрточные нейронные сети (CNN).
    \item Рекуррентные нейронные сети (RNN).
    \item Комбинация свёрточной и рекуррентной сетей с CTC на выходе (CNN-RNN-CTC).
    \item Sequence-to-sequence модели (seq2seq).
    \item Трансформеры.
\end{itemize}

